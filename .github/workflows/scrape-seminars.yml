name: セミナー情報スクレイピング

# いつ実行するか
on:
  # 定期実行：4時間ごと
  schedule:
    - cron: '0 */4 * * *'  # 0時、4時、8時、12時、16時、20時に実行
  
  # 手動実行も可能にする
  workflow_dispatch:
  
  # mainブランチへのプッシュ時も実行（テスト用）
  push:
    branches: [ main ]
    paths:
      - 'scraper/**'  # scraperフォルダ内のファイルが変更された時のみ

# 実行する処理
jobs:
  scrape:
    runs-on: ubuntu-latest  # Ubuntu環境で実行
    
    steps:
    # 1. リポジトリのコードを取得
    - name: リポジトリをチェックアウト
      uses: actions/checkout@v3
    
    # 2. Pythonをセットアップ
    - name: Python 3.9をセットアップ
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    # 3. Chromeブラウザをインストール
    - name: Chromeをインストール
      run: |
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable
    
    # 4. Python依存関係をインストール
    - name: 依存関係をインストール
      run: |
        python -m pip install --upgrade pip
        pip install selenium beautifulsoup4 pandas tqdm python-dotenv supabase chromedriver-binary
    
    # 5. スクレイピングを実行
    - name: セミナー情報をスクレイピング
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
      run: |
        echo "スクレイピング処理を開始します..."
        echo "対象URL: https://expertspa.biz/seminar"
        echo "==============================="
        python scraper/main_with_update.py 2>&1 | tee scraping.log
        echo "==============================="
        echo "スクレイピング処理が完了しました"
    
    # 6. デバッグ情報を表示
    - name: スクレイピング結果の要約
      if: always()
      run: |
        echo "=== スクレイピング結果要約 ==="
        if [ -f scraping.log ]; then
          echo "ログファイルから結果を抽出..."
          grep -E "(追加|更新|削除|エラー|成功|失敗)" scraping.log || echo "特定のパターンが見つかりませんでした"
        else
          echo "ログファイルが見つかりません"
        fi
        echo "==============================="
        echo "実行時刻: $(date)"
        echo "ステータス: ${{ job.status }}"
        
    # 7. エラーログの詳細
    - name: エラーログを確認
      if: failure()
      run: |
        echo "=== エラーログ ==="
        if [ -f scraping.log ]; then
          tail -50 scraping.log
        fi